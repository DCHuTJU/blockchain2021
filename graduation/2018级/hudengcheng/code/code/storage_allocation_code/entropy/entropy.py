from normalization.normalization import mapminmax
import math


def entropy_calculation(set):
    result = []

    # 第一步 归一化
    # 直接选择使用最大-最小值
    # for i in range(len(set)):
    #     for j in range(len(set[i])):
    #         set[i][j] = normalization_with_number(set[i][j])

    data_availability = []
    data_storage_time = []
    data_cost = []
    for i in range(len(set)):
        data_availability.append(set[i][0])
        data_storage_time.append(set[i][1])
        data_cost.append(set[i][2])


    # 第二种归一化方法
    # print(max(data_availability), min(data_availability))

    # max_data_availability = max(data_availability)+0.001
    # min_data_availability = min(data_availability)-0.001
    # max_data_storage_time = max(data_storage_time)+1
    # min_data_storage_time = min(data_storage_time)-1
    # max_data_cost = max(data_cost)+0.01
    # min_data_cost = min(data_cost)-0.01
    #
    # # print(max(data_storage_time), min(data_storage_time))
    # # print(max(data_cost), min(data_cost))
    # for i in range(len(data_availability)):
    #     data_availability[i] = (data_availability[i] - min_data_availability) / (max_data_availability - min_data_availability)
    #
    # for i in range(len(data_storage_time)):
    #
    #     data_storage_time[i] = (data_storage_time[i] - min_data_storage_time) / (max_data_storage_time - min_data_storage_time)
    #
    # for i in range(len(data_cost)):
    #
    #     data_cost[i] = (data_cost[i] - min_data_cost) / (max_data_cost - min_data_cost)

    data_availability = mapminmax(data_availability, 0.01, 1)
    data_storage_time = mapminmax(data_storage_time, 0.01, 1)
    data_cost = mapminmax(data_cost, 0.01, 1)

    for i in range(len(set)):
        set[i][0] = data_availability[i]
        set[i][1] = data_storage_time[i]
        set[i][2] = data_cost[i]

    print(set)




    # 第二步 计算索引系数
    A_0 = []
    A_1 = []
    A_2 = []
    for i in range(len(set)):
        A_0.append(set[i][0])
        A_1.append(set[i][1])
        A_2.append(set[i][2])

    for i in range(len(set)):
        result.append([set[i][0]/sum(A_0), set[i][1]/sum(A_1), set[i][2]/sum(A_2)])
    # print(result)

    # 第三步 计算熵值
    A_0_ = []
    A_1_ = []
    A_2_ = []


    for i in range(len(result)):
        A_0_.append(result[i][0])
        A_1_.append(result[i][1])
        A_2_.append(result[i][2])

    # print(A_0_)
    # print(A_1_)
    # print(A_2_)

    for i in range(len(result)):
        A_0_[i] = A_0_[i] * math.log(A_0_[i])
        A_1_[i] = A_1_[i] * math.log(A_1_[i])
        A_2_[i] = A_2_[i] * math.log(A_2_[i])

    entropy_set = [0] * 3
    k = 1 / math.log(len(set))
    entropy_set[0] = -1 * k * sum(A_0_)
    entropy_set[1] = -1 * k * sum(A_1_)
    entropy_set[2] = -1 * k * sum(A_2_)

    for i in range(len(entropy_set)):
        entropy_set[i] = 1 - entropy_set[i]

    sum_entropy = sum(entropy_set)
    weights = []
    weights.append(entropy_set[0] / sum_entropy)
    weights.append(entropy_set[1] / sum_entropy)
    weights.append(entropy_set[2] / sum_entropy)

    qos_set = []

    for i in range(len(set)):
        tmp = 0
        tmp += weights[0] * set[i][0]
        tmp += weights[1] * set[i][1]
        tmp += weights[2] * set[i][2]
        qos_set.append(tmp)

    return qos_set

set = [[0.0028670808431353486, 71.97731055519792, 0.03357350431023234], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0018583117201001315, 76.33058974038796, 0.025121556240648955], [0.0017543657422328994, 72.03399125498152, 0.02863480777248075], [0.0017543657422328994, 72.03399125498152, 0.02863480777248075], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.002605626406926287, 72.88587675662666, 0.028349998948868728], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.002688713374161313, 76.33058974038796, 0.027682536688082515], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0028670808431353486, 71.97731055519792, 0.03357350431023234], [0.0018583117201001315, 76.33058974038796, 0.025121556240648955], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656], [0.0014467882466736436, 72.88587675662666, 0.02641936690754656]]

rlt = entropy_calculation(set)

print(rlt)